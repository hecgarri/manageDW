devtools::document()
library(manageDW)
#help(package = "manageDW")
cargar_paquetes(c("RSelenium","rvest", "tidyverse"))
devtools::document()
devtools::install()
devtools::document()
devtools::install()
remove.packages("manageDW")
devtools::document()
devtools::install()
library(manageDW)
library(manageDW)
cargar_paquetes()
library(manageDW)
#help(package = "manageDW")
cargar_paquetes(c("RSelenium","rvest", "tidyverse"))
# 1. Iniciar Selenium con Chromedriver en modo headless
rD <- rsDriver(
browser = "chrome",
chromever = "latest",
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
# 2. Navegar al sitio del Servel
url_base <- "https://elecciones.servel.cl/"
remDr$navigate(url_base)
library(manageDW)
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
log_info("=== Inicio de ejecución ===")
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
log_info("Cerrando Selenium...")
scraper$cerrar()
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
log_info("Cerrando Selenium...")
scraper$cerrar()
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
log_info("Cerrando Selenium...")
scraper$cerrar()
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
# ============================================================================
# 2. Clase de Scraper Servel
# ============================================================================
ServelScraper <- R6::R6Class("ServelScraper",
public = list(
remDr = NULL,
rD = NULL,
initialize = function() {
message("Iniciando Selenium...")
self$rD <- rsDriver(
browser = "chrome",
chromever = "latest",
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
self$remDr <- self$rD$client
},
navegar_base = function(url = "https://elecciones.servel.cl/") {
message("Navegando a: ", url)
self$remDr$navigate(url)
Sys.sleep(3)
},
obtener_resultados = function() {
message("Obteniendo resultados...")
# Ejemplo: extraer tabla principal, ajustar selector real según inspección
html <- self$remDr$getPageSource()[[1]]
page <- read_html(html)
tabla <- page %>%
html_node("table") %>% # cambia selector según corresponda
html_table(fill = TRUE)
return(tabla)
},
cerrar = function() {
message("Cerrando Selenium...")
self$remDr$close()
self$rD$server$stop()
}
)
)
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
log_info("Cerrando Selenium...")
scraper$cerrar()
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
# ============================================================================
# 2. Clase de Scraper Servel
# ============================================================================
ServelScraper <- R6::R6Class("ServelScraper",
public = list(
remDr = NULL,
rD = NULL,
initialize = function() {
message("Iniciando Selenium...")
self$rD <- rsDriver(
browser = "chrome",
chromever = "latest",
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
self$remDr <- self$rD$client
},
navegar_base = function(url = "https://elecciones.servel.cl/") {
message("Navegando a: ", url)
self$remDr$navigate(url)
Sys.sleep(3)
},
obtener_resultados = function() {
message("Obteniendo resultados...")
# Ejemplo: extraer tabla principal, ajustar selector real según inspección
html <- self$remDr$getPageSource()[[1]]
page <- read_html(html)
tabla <- page %>%
html_node("table") %>% # cambia selector según corresponda
html_table(fill = TRUE)
return(tabla)
},
cerrar = function() {
message("Cerrando Selenium...")
self$remDr$close()
self$rD$server$stop()
}
)
)
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
log_info("Cerrando Selenium...")
scraper$cerrar()
ServelScraper
View(ServelScraper)
class(ServelScraper)
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
# ============================================================================
# 2. Clase de Scraper Servel
# ============================================================================
ServelScraper <- R6::R6Class("ServelScraper",
public = list(
remDr = NULL,
rD = NULL,
initialize = function() {
message("Iniciando Selenium...")
self$rD <- rsDriver(
browser = "chrome",
chromever = "latest",
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
self$remDr <- self$rD$client
},
navegar_base = function(url = "https://elecciones.servel.cl/") {
message("Navegando a: ", url)
self$remDr$navigate(url)
Sys.sleep(3)
},
obtener_resultados = function() {
message("Obteniendo resultados...")
# Ejemplo: extraer tabla principal, ajustar selector real según inspección
html <- self$remDr$getPageSource()[[1]]
page <- read_html(html)
tabla <- page %>%
html_node("table") %>% # cambia selector según corresponda
html_table(fill = TRUE)
return(tabla)
},
cerrar = function() {
message("Cerrando Selenium...")
self$remDr$close()
self$rD$server$stop()
}
)
)
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
log_info("Cerrando Selenium...")
scraper$cerrar()
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger"))
# ============================================================================
# Script completo: Scraper Servel con logging robusto
# ============================================================================
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger", "R6"))
# ============================================================================
# Configuración de logger
# ============================================================================
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
# ============================================================================
# Clase de Scraper Servel
# ============================================================================
ServelScraper <- R6::R6Class("ServelScraper",
public = list(
remDr = NULL,
rD = NULL,
initialize = function() {
log_info("Iniciando Selenium...")
self$rD <- rsDriver(
browser = "chrome",
chromever = "latest",
check = FALSE,  # evita buscar phantomjs
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
self$remDr <- self$rD$client
},
navegar_base = function(url = "https://elecciones.servel.cl/") {
log_info("Navegando a: {url}")
self$remDr$navigate(url)
Sys.sleep(3)
},
obtener_resultados = function() {
log_info("Obteniendo resultados...")
html <- self$remDr$getPageSource()[[1]]
page <- read_html(html)
tabla <- page %>%
html_node("table") %>% # ajustar selector según estructura real
html_table(fill = TRUE)
return(tabla)
},
cerrar = function() {
log_info("Cerrando Selenium...")
self$remDr$close()
self$rD$server$stop()
}
)
)
# ============================================================================
# Ejecución del scraping con manejo de errores
# ============================================================================
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
# ============================================================================
# Cierre de Selenium si scraper existe
# ============================================================================
if (exists("scraper")) {
scraper$cerrar()
}
log_success("=== Script finalizado sin errores críticos ===")
# ============================================================================
# Script completo: Scraper Servel con logging robusto
# ============================================================================
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger", "R6"))
# ============================================================================
# Configuración de logger
# ============================================================================
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
# ============================================================================
# Clase de Scraper Servel
# ============================================================================
ServelScraper <- R6::R6Class("ServelScraper",
public = list(
remDr = NULL,
rD = NULL,
initialize = function() {
log_info("Iniciando Selenium...")
self$rD <- rsDriver(
browser = "chrome",
chromever = "latest",
check = FALSE,  # evita buscar phantomjs
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
self$remDr <- self$rD$client
},
navegar_base = function(url = "https://elecciones.servel.cl/") {
log_info("Navegando a: {url}")
self$remDr$navigate(url)
Sys.sleep(3)
},
obtener_resultados = function() {
log_info("Obteniendo resultados...")
html <- self$remDr$getPageSource()[[1]]
page <- read_html(html)
tabla <- page %>%
html_node("table") %>% # ajustar selector según estructura real
html_table(fill = TRUE)
return(tabla)
},
cerrar = function() {
log_info("Cerrando Selenium...")
self$remDr$close()
self$rD$server$stop()
}
)
)
# ============================================================================
# Ejecución del scraping con manejo de errores
# ============================================================================
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
# ============================================================================
# Cierre de Selenium si scraper existe
# ============================================================================
if (exists("scraper")) {
scraper$cerrar()
}
log_success("=== Script finalizado sin errores críticos ===")
# ============================================================================
# Script completo: Scraper Servel con logging robusto
# ============================================================================
library(manageDW)
cargar_paquetes(c("RSelenium", "rvest", "tidyverse", "logger", "R6"))
# ============================================================================
# Configuración de logger
# ============================================================================
log_layout(layout_glue_colors)
log_threshold(INFO)
log_info("=== Inicio de ejecución ===")
# ============================================================================
# Clase de Scraper Servel
# ============================================================================
ServelScraper <- R6::R6Class("ServelScraper",
public = list(
remDr = NULL,
rD = NULL,
initialize = function() {
log_info("Iniciando Selenium...")
self$rD <- rsDriver(
browser = "chrome",
chromever = "latest",
check = FALSE,  # evita buscar phantomjs
extraCapabilities = list(
chromeOptions = list(args = c("--headless", "--no-sandbox", "--disable-gpu"))
)
)
self$remDr <- self$rD$client
},
navegar_base = function(url = "https://elecciones.servel.cl/") {
log_info("Navegando a: {url}")
self$remDr$navigate(url)
Sys.sleep(3)
},
obtener_resultados = function() {
log_info("Obteniendo resultados...")
html <- self$remDr$getPageSource()[[1]]
page <- read_html(html)
tabla <- page %>%
html_node("table") %>% # ajustar selector según estructura real
html_table(fill = TRUE)
return(tabla)
},
cerrar = function() {
log_info("Cerrando Selenium...")
self$remDr$close()
self$rD$server$stop()
}
)
)
# ============================================================================
# Ejecución del scraping con manejo de errores
# ============================================================================
tryCatch({
log_info("Inicializando scraper Servel...")
scraper <- ServelScraper$new()
log_info("Navegando al sitio...")
scraper$navegar_base()
log_info("Extrayendo resultados...")
resultados <- scraper$obtener_resultados()
log_success("Extracción completada.")
log_info("Guardando resultados...")
write_csv(resultados, "resultados_servel.csv")
log_success("Resultados guardados exitosamente.")
}, error = function(e) {
log_error("Se produjo un error: {e$message}")
})
# ============================================================================
# Cierre de Selenium si scraper existe
# ============================================================================
if (exists("scraper")) {
scraper$cerrar()
}
log_success("=== Script finalizado sin errores críticos ===")
binman::rm_platform("phantomjs")
